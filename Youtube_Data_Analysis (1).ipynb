{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtINOmZZ9T3l"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import gdown\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import*\n",
        "from pyspark.sql.functions import date_format, col, count, when, regexp_extract, avg, var_pop, format_number, min, max, first, last, count_distinct, asc, sum\n",
        "from datetime import date\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, PCA, MinMaxScaler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.sql.types import IntegerType\n",
        "from os.path import join\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "\n",
        "#LEITURA ESCRITA: _LE\n",
        "\n",
        "spark= SparkSession.builder.getOrCreate()\n",
        "\n",
        "#videos-stats\n",
        "url_LE='https://drive.google.com/file/d/1qoYIh2XjG_Gqy5dA6fVwAVqZW0PuIf7R/view?usp=drive_link'\n",
        "\n",
        "# Extração do arquivo pela URL\n",
        "file_id_LE = url_LE.split('/')[-2]\n",
        "\n",
        "# Download do arquivo\n",
        "output_file_LE = 'videos-stats.csv'\n",
        "gdown.download(id=file_id_LE, output=output_file_LE, quiet=False)\n",
        "\n",
        "# Leitura do arquivo CSV usando spark\n",
        "df = spark.read.csv(output_file_LE)\n",
        "\n",
        "#Visualize os primeiros 8 registro do arquivo\n",
        "df.show(8)\n",
        "\n",
        "#Visualize o esquema do arquivo\n",
        "df.printSchema()\n",
        "\n",
        "#Leia novamente o arquivo inferindo o esquema e visualize o esquema novamente\n",
        "\n",
        "df = spark.read.csv(output_file_LE, header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "#Salve o arquivo como 'videos-parquet' no formato parquet e adicione o cabeçalho nos dados\n",
        "\n",
        "df.write.option('header','true').mode(\"overwrite\").parquet('videos-parquet')\n",
        "\n",
        "\n",
        "#Leia e visualize o arquivo 'videos-parquet' com cabeçalho nos dados;\n",
        "\n",
        "df = spark.read.parquet('videos-parquet')\n",
        "df.show()\n",
        "\n",
        "#Salve o arquivo do exec. anterior como tabela chamada ‘tb_videos’ no banco de dados default do spark catalog\n",
        "\n",
        "df.write.option('header','true').mode(\"overwrite\").saveAsTable('tb_videos')\n",
        "\n",
        "#Liste as tabelas do spark catalog para verificar a tabela\n",
        "\n",
        "spark.catalog.listTables()\n",
        "\n",
        "#Utilize o spark SQL para ler a tabela ‘tb_videos’\n",
        "\n",
        "tab_df = spark.sql('select * from tb_videos')\n",
        "tab_df.show()\n",
        "\n",
        "#Leia o arquivo ‘comments.csv' inferindo o esquema e visualize\n",
        "\n",
        "url_LE2='https://drive.google.com/file/d/1NqytO5N-riQGo9-puSFsDnaWtRzKA5dv/view?usp=drive_link'\n",
        "\n",
        "# Extração do arquivo pela URL\n",
        "file_id_LE2 = url_LE2.split('/')[-2]\n",
        "\n",
        "# Download do arquivo\n",
        "output_file_LE2 = 'comments.csv'\n",
        "gdown.download(id=file_id_LE2, output=output_file_LE2, quiet=False)\n",
        "\n",
        "#Leitura do arquivo\n",
        "df = spark.read.csv(output_file_LE2, header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "\n",
        "#Salve o arquivo como ‘comments-parquet' no formato parquet e adicione o cabeçalho nos dados\n",
        "\n",
        "df.write.option('header','true').mode(\"overwrite\").parquet('comments-parquet')\n",
        "\n",
        "\n",
        "#TRATAMENTO : _TR\n",
        "\n",
        "#Leia o arquivo ‘videos-stats.csv'  no dataframe 'df_video' com cabeçalho e inferindo o esquema\n",
        "df_video_url_TR='https://drive.google.com/file/d/1XAefeuunm6q7o0OEdF--NzEPXTmIG9i5/view?usp=drive_link'\n",
        "# Extração do arquivo pela URL\n",
        "file_id_TR = df_video_url_TR.split('/')[-2]\n",
        "# Download do arquivo\n",
        "output_file_TR = 'videos-stats.csv'\n",
        "gdown.download(id=file_id_TR, output=output_file_TR, quiet=False)\n",
        "#Leitura do arquivo inferindo o esquema\n",
        "df_video_TR= spark.read.csv(output_file_TR, header=True, inferSchema=True)\n",
        "df_video_TR.printSchema()\n",
        "\n",
        "#Altere os valores nulos dos campos 'Likes', 'Comments' e 'Views' para o valor 0\n",
        "\n",
        "df_video_TR = df_video_TR.fillna({'Likes': 0, 'Comments': 0, 'Views': 0})\n",
        "df_video_TR.show()\n",
        "\n",
        "#Leia o arquivo ‘comments.csv' no dataframe 'df_comentario' com cabeçalho e inferindo o esquema\n",
        "\n",
        "df_comentario_url_TR='https://drive.google.com/file/d/1rW-4ufh6UqafMAeRrH1MVypHkS75lfFM/view?usp=drive_link'\n",
        "\n",
        "# Extração do arquivo pela URL\n",
        "file_id_TR = df_comentario_url_TR.split('/')[-2]\n",
        "\n",
        "# Download do arquivo\n",
        "output_file_TR = 'comments.csv'\n",
        "gdown.download(id=file_id_TR, output=output_file_TR, quiet=False)\n",
        "\n",
        "#Leitura do arquivo\n",
        "df_comentario_TR = spark.read.csv(output_file_TR, header=True, inferSchema=True)\n",
        "df_comentario_TR.printSchema()\n",
        "\n",
        "#Calcule a quantidade de registros do df_video e df_comentario\n",
        "\n",
        "df_video_contagem_TR  = df_video_TR.count()\n",
        "df_comentario_contagem_TR = df_comentario_TR.count()\n",
        "\n",
        "print(\"Quantidade de registros em df_video:\", df_video_contagem_TR)\n",
        "print(\"Quantidade de registros em df_comentario:\", df_comentario_contagem_TR)\n",
        "\n",
        "#Remova os registros do df_video e df_comentario quem possuem o campo 'Video ID' nulos e calcule novamente a quantidade de registros\n",
        "\n",
        "df_video_TR = df_video_TR.dropna(subset=['Video ID'])\n",
        "df_comentario_TR = df_comentario_TR.dropna(subset=['Video ID'])\n",
        "\n",
        "df_video_contagem_TR  = df_video_TR.count()\n",
        "df_comentario_contagem_TR = df_comentario_TR.count()\n",
        "\n",
        "print(\"Quantidade de registros sem nulos em df_video:\", df_video_contagem_TR)\n",
        "print(\"Quantidade de registros sem nulos em df_comentario:\", df_comentario_contagem_TR)\n",
        "\n",
        "#Remova os registros apenas do df_video quem possuem o campo 'Video ID' duplicados\n",
        "\n",
        "df_video_TR = df_video_TR.dropDuplicates(['Video ID'])\n",
        "df_video_contagem_TR  = df_video_TR.count()\n",
        "\n",
        "print(\"Quantidade de registros sem duplicados em df_video:\", df_video_contagem_TR)\n",
        "\n",
        "#Converta os campos Likes, Comments e Views para 'int' no dataframe df_video\n",
        "\n",
        "df_video_TR = df_video_TR.withColumn(\"Likes\", df_video_TR[\"Likes\"].cast(IntegerType()))\n",
        "\n",
        "#Converta os campos Likes e Sentiment para 'int' no dataframe df_comentario, além disso, altere o nome do campo Likes para 'Likes Comment'\n",
        "\n",
        "df_comentario_TR = df_comentario_TR.withColumn(\"Likes\", df_comentario_TR[\"Likes\"].cast(IntegerType()))\n",
        "df_comentario_TR = df_comentario_TR.withColumn(\"Sentiment\", df_comentario_TR[\"Sentiment\"].cast(IntegerType()))\n",
        "df_comentario_TR = df_comentario_TR.withColumnRenamed(\"Likes\", \"Likes Comment\")\n",
        "\n",
        "#Crie o campo 'Interaction' no dataframe df_video, com a soma dos campos Likes, Comments e Views\n",
        "\n",
        "df_video_TR = df_video_TR.withColumn(\"Interaction\", df_video_TR[\"Likes\"] + df_video_TR[\"Comments\"] + df_video_TR[\"Views\"])\n",
        "print(df_video_TR.show())\n",
        "\n",
        "#Converta os campos 'Published At' para 'date' no dataframe df_video\n",
        "\n",
        "df_video_TR = df_video_TR.withColumn(\"Published At\", df_video_TR[\"Published At\"].cast(DateType()))\n",
        "\n",
        "#Crie o campo 'Year' no dataframe df_video, extraindo apenas o ano do campo 'Published At'\n",
        "\n",
        "df_video_TR = df_video_TR.withColumn(\"Year\", date_format(df_video_TR[\"Published At\"], \"yyyy\"))\n",
        "\n",
        "#Mescle os dados df_comentario no dataframe df_video em relação ao campo Video ID e crie o dataframe df_join_video_comments\n",
        "\n",
        "mescla_video_comentario_TR = join(\"df_video_TR\", \"df_comentario_TR\")\n",
        "df_join_video_comments_TR = df_video_TR.join(df_comentario_TR, on=\"Video ID\", how=\"left\")\n",
        "\n",
        "#Leia o arquivo ‘videos-stats.csv'  no dataframe 'df_video' com cabeçalho e inferindo o esquema\n",
        "df_us_videos_url_TR='https://drive.google.com/file/d/1NmT0RoxSv9GSJ08qDIMdeRkwbsaXzJE_/view?usp=drive_link'\n",
        "# Extração do arquivo pela URL\n",
        "file_id_TR = df_us_videos_url_TR.split('/')[-2]\n",
        "# Download do arquivo\n",
        "output_file_TR = 'USvideos.csv.csv'\n",
        "gdown.download(id=file_id_TR, output=output_file_TR, quiet=False)\n",
        "#Leitura do arquivo inferindo o esquema\n",
        "df_us_videos_TR= spark.read.csv(output_file_TR, header=True, inferSchema=True)\n",
        "df_us_videos_TR.printSchema()\n",
        "\n",
        "#Mescle os dados df_us_videos no dataframe df_video em relação ao campo Title e crie e visualize o dataframe df_join_video_usvideos\n",
        "\n",
        "mescla_us_videos_video_TR = join(\"df_video_TR\", \"df_us_videos_TR\")\n",
        "df_join_video_usvideos_TR = df_video_TR.join(df_us_videos_TR, on=\"Title\", how=\"left\")\n",
        "df_join_video_usvideos_TR.show()\n",
        "\n",
        "#Verifique a quantidade de campos nulos em todos os campos do dataframe df_video\n",
        "\n",
        "df_video_nulos_TR = df_video_TR.select([count(when(col(c).isNull(), c)).alias(c) for c in df_video_TR.columns])\n",
        "df_video_nulos_TR.show()\n",
        "\n",
        "#Remova a coluna '_c0' e salve o dataframe df_video como 'videos-tratados-parquet' no formato parquet e adicione o cabeçalho nos dados\n",
        "\n",
        "df_video_TR = df_video_TR.drop(\"_c0\")\n",
        "df_video_TR.write.option('header','true').mode(\"overwrite\").parquet('videos-tratados-parquet')\n",
        "\n",
        "#Remova a coluna '_c0' e salve o dataframe df_join_video_comments como 'videos-comments-tratados-parquet' no formato parquet e adicione o cabeçalho nos dados\n",
        "\n",
        "df_join_video_comments_TR = df_join_video_comments_TR.drop(\"_c0\")\n",
        "df_join_video_comments_TR.write.option('header','true').mode(\"overwrite\").parquet('videos-comments-tratados-parquet')\n",
        "\n",
        "#PREPARAÇÃO : _PR\n",
        "\n",
        "#Leia o arquivo ‘videos-tratados.snappy.parquet' no dataframe 'df_video'\n",
        "\n",
        "df_video_PR = videos-tratados.snappy.parquet\n",
        "\n",
        "#Adicione a coluna 'Month' com o valor do mês da coluna \"Published At\"\n",
        "\n",
        "df_video_PR = df_video_PR.withColumn('Month', date_format(col('Published At'), 'MM').cast('int')) #Ajustado de 'mmmm' para 'mm' e colocado como inteiro\n",
        "\n",
        "#Adicione a coluna \"Keyword Index\" com a transformação da coluna 'keyword' para valores numéricos\n",
        "\n",
        "indexer_PR = StringIndexer(inputCol='Keyword', outputCol='Keyword Index')\n",
        "df_video_PR = indexer.fit(df_video_PR).transform(df_video_PR)\n",
        "\n",
        "#Crie um vetor chamado \"Features\" com os campos: \"Likes\", \"Views\", \"Year\", \"Month\", \"Keyword Index\" e transforme o dataframe df_video com o VectorAssembler, lembrando que o vetor só aceita campos do tipo numérico\n",
        "\n",
        "df_video_PR = df_video_PR.withColumn(\"Year\", df_video_PR[\"Year\"].cast(IntegerType())) ## Convertido para inteiro\n",
        "df_video_PR = df_video_PR.na.drop()\n",
        "montar_vetor_PR = VectorAssembler(inputCols=['Likes', 'Views', 'Year', 'Month', 'Keyword Index'],\n",
        "                               outputCol='Features')\n",
        "df_video_PR = montar_vetor_PR.transform(df_video_PR)\n",
        "\n",
        "#Adicione a coluna \"Features Normal\" com os dados normalizados da coluna Features, lembrando que para normalizar a coluna não pode conter valores nulos\n",
        "\n",
        "df_video_PR = df_video_PR.na.drop()\n",
        "\n",
        "scaler_PR= MinMaxScaler(inputCol='Features', outputCol='Features Normal')\n",
        "modelo_scaler_PR = scaler_PR.fit(df_video_PR)\n",
        "df_video_PR = modelo_scaler_PR.transform(df_video_PR)\n",
        "\n",
        "#Adicione a coluna \"Features PCA\" com a redução de 5 características para 1, utilizando o modelo PCA\n",
        "\n",
        "pca_PR = PCA(k=1, inputCol='Features Normal', outputCol='Features PCA')\n",
        "modelo_pca_PR = pca_PR.fit(df_video_PR)\n",
        "df_video_PR = modelo_pca_PR.transform(df_video_PR)\n",
        "\n",
        "#Separe o dataframe df_video em 2 conjuntos: 80% para treinamento e 20% para teste\n",
        "\n",
        "df_treino_PR, df_teste_PR = df_video_PR.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "#Crie um modelo de regressão linear para estimar o valor do campo \"Comments\", utilizando a \"Features Normal\" e avalie o modelo\n",
        "\n",
        "regressão_linear_PR = LinearRegression(featuresCol='Features Normal', labelCol='Comments')\n",
        "modelo_rl_PR = regressão_linear_PR.fit(df_treino_PR)\n",
        "\n",
        "avaliacao_rl_PR = modelo_rl_PR.evaluate(df_teste_PR)\n",
        "print(\"RMSE(Erro quadrático médio da raiz):\", avaliacao_rl_PR.rootMeanSquaredError)\n",
        "print(\"R2(Coeficiente de determinação):\", avaliacao_rl_PR.r2)\n",
        "\n",
        "#Salve o dataframe df_video como 'videos-preparados-parquet' no formato parquet\n",
        "\n",
        "df_video_PR.write.option('header','true').mode(\"overwrite\").parquet('videos-preparados.snappy.parquet')\n",
        "\n",
        "\n",
        "#AGREGAÇÃO : _AG\n",
        "\n",
        "#Leia o arquivo 'videos-preparados.snappy.parquet' no dataframe 'df_video'\n",
        "\n",
        "df_video_url_AG='https://drive.google.com/file/d/145CE3eezLNunzvr8aofldQq4wTlBaCsR/view?usp=sharing'\n",
        "# Extração do arquivo pela URL\n",
        "file_id_AG = \"145CE3eezLNunzvr8aofldQq4wTlBaCsR\"\n",
        "# Download do arquivo\n",
        "output_file_AG = 'videos-preparados.snappy.parquet'\n",
        "gdown.download(id=file_id_AG, output=output_file_AG, quiet=False, fuzzy=True)\n",
        "#Leitura do arquivo inferindo o esquema\n",
        "df_video_AG = spark.read.parquet(output_file_AG)\n",
        "\n",
        "#Calcule a quantidade de registros para cada valor único da coluna \"Keyword\"\n",
        "\n",
        "df_video_AG.groupBy(\"Keyword\").count().show()\n",
        "\n",
        "#Calcule a média da coluna \"Interaction\" para cada valor único da coluna 'Keyword'\n",
        "\n",
        "df_video_AG.groupBy(\"Keyword\").avg(\"Interaction\").show()\n",
        "\n",
        "#Calcule o valor máximo da coluna \"Interaction\" para cada valor único da coluna 'Keyword' e nomeie de 'Rank Interactions', em seguida ordene pela nova coluna em ordem decrescente\n",
        "\n",
        "df_video_AG.groupBy(\"Keyword\").max(\"Interaction\").withColumnRenamed(\"max(Interaction)\", \"Rank Interactions\").orderBy(\"Rank Interactions\", ascending=False).show()\n",
        "\n",
        "#Calcule a média e a variância da coluna 'Views' para cada valor único da coluna 'Keyword'\n",
        "\n",
        "df_video_AG.groupBy(\"Keyword\").agg(avg(\"Views\"), var_pop(\"Views\")).show()\n",
        "\n",
        "#Calcule a média, o valor mínimo e o valor máximo de 'Views' para cada valor único da coluna 'Keyword', sem casas decimais\n",
        "\n",
        "df_video_AG.groupBy(\"Keyword\").agg(\n",
        "    format_number(avg(\"Views\"), 2).alias(\"avg_views\"),\n",
        "    format_number(min(\"Views\"), 2).alias(\"min_views\"),\n",
        "    format_number(max(\"Views\"), 2).alias(\"max_views\")\n",
        ").show()\n",
        "\n",
        "#Mostre o primeiro e o último 'Published At' para cada valor único da coluna 'Keyword'\n",
        "\n",
        "df_video_AG.groupBy(\"Keyword\").agg(first(\"Published At\"), last(\"Published At\")).show()\n",
        "\n",
        "#Conte todos os 'title' de forma normal e todos os únicos e verifique se há diferença\n",
        "\n",
        "df_video_AG.groupBy(\"title\").agg(\n",
        "      count(\"title\").alias(\"Contagem normal\"),\n",
        "      count_distinct(\"title\").alias(\"Contagem unicos\")\n",
        "      ).show()\n",
        "\n",
        "#Mostre a quantidade de registros ordenados por ano em ordem ascendente\n",
        "\n",
        "df_video_AG.groupBy(df_video_AG[\"Published At\"].substr(1, 4).alias(\"Ano\")).count().orderBy(\"Ano\").show()\n",
        "\n",
        "#Mostre a quantidade de registros ordenados por ano e mês em ordem ascendente\n",
        "\n",
        "df_video_AG.groupBy(df_video_AG[\"Published At\"].substr(1, 4).alias(\"Ano\"),\n",
        "                 df_video_AG[\"Published At\"].substr(6, 2).alias(\"Mes\")).count().orderBy(\"Ano\", \"Mes\").show()\n",
        "\n",
        "#Calcule a média acumulativa de ‘Likes’ para cada ‘Keyword’ ao longo dos anos\n",
        "\n",
        "df_video_AG = df_video_AG.withColumn(\"Ano\", df_video_AG[\"Published At\"].substr(1, 4))\n",
        "window_spec_AG = Window.partitionBy(\"Keyword\").orderBy(\"Ano\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "df_video_AG = df_video_AG.withColumn(\"Media Acumulada Likes\", format_number(avg(\"Likes\").over(window_spec_AG), 0))\n",
        "df_video_AG.show()\n",
        "\n",
        "#OTIMIZAÇÃO : _OT\n",
        "\n",
        "#Leia o arquivo 'videos-preparados.snappy.parquet' no dataframe 'df_video'\n",
        "\n",
        "df_video_url_OT='https://drive.google.com/file/d/145CE3eezLNunzvr8aofldQq4wTlBaCsR/view?usp=drive_link'\n",
        "# Extração do arquivo pela URL\n",
        "file_id_OT = df_video_url_OT.split('/')[-2]\n",
        "# Download do arquivo\n",
        "output_file_OT = 'videos-preparados.snappy.parquet'\n",
        "gdown.download(id=file_id_OT, output=output_file_OT, quiet=False)\n",
        "#Leitura do arquivo inferindo o esquema\n",
        "df_video_OT = spark.read.parquet(output_file_OT)\n",
        "\n",
        "#Leia o arquivo ‘video-comments-tratados.snappy.parquet' no dataframe 'df_comments\n",
        "\n",
        "df_comments_url_OT='https://drive.google.com/file/d/1e3CYHGCNb2INUt-w0iMQM-9ZT5QPQ6eV/view?usp=drive_link'\n",
        "# Extração do arquivo pela URL\n",
        "file_id_OT = df_comments_url_OT.split('/')[-2]\n",
        "# Download do arquivo\n",
        "output_file_OT = 'video-comments-tratados.snappy.parquet'\n",
        "gdown.download(id=file_id_OT, output=output_file_OT, quiet=False)\n",
        "#Leitura do arquivo inferindo o esquema\n",
        "df_comments_OT = spark.read.parquet(output_file_OT)\n",
        "\n",
        "# Renomear colunas para evitar espaços e inconsistências\n",
        "df_video_cleaned_OT = df_video_OT.withColumnRenamed(\"Video ID\", \"video_id\")\n",
        "df_comments_cleaned_OT = df_comments_OT.withColumnRenamed(\"Video ID\", \"video_id\")\n",
        "\n",
        "#Crie tabelas temporárias para ambos os dataframe\n",
        "\n",
        "df_video_cleaned_OT.createOrReplaceTempView('video_table')\n",
        "df_comments_cleaned_OT.createOrReplaceTempView('comments_table')\n",
        "\n",
        "#Faça um join das tabelas criadas anteriormente utilizando o spark.sql no dataframe ‘join_video_comments\n",
        "\n",
        "join_video_comments_OT = spark.sql(\"\"\"\n",
        "    SELECT v.*, c.*\n",
        "    FROM video_table v\n",
        "    JOIN comments_table c\n",
        "    ON v.video_id = c.video_id\n",
        "\"\"\")\n",
        "\n",
        "join_video_comments_OT.show()\n",
        "\n",
        "#Faça as mesmas etapas anteriores utilizando repartition e coalesce\n",
        "\n",
        "# Renomeando a coluna 'Video ID' para 'video_id' no DataFrame df_video\n",
        "df_video_cleaned_OT = df_video_OT.withColumnRenamed(\"Video ID\", \"video_id\")\n",
        "# Renomeando a coluna 'Video ID' para 'video_id' no DataFrame df_comments\n",
        "df_comments_cleaned_OT = df_comments_OT.withColumnRenamed(\"Video ID\", \"video_id\")\n",
        "\n",
        "##  Realizar o JOIN com reparticionamento\n",
        "df_video_repartition_OT = df_video_cleaned_OT.repartition(10)\n",
        "df_comments_repartition_OT = df_comments_cleaned_OT.repartition(10)\n",
        "\n",
        "# Criar tabelas temporárias novamente\n",
        "df_video_repartition_OT.createOrReplaceTempView(\"video_table_repartition\")\n",
        "df_comments_repartition_OT.createOrReplaceTempView(\"comments_table_repartition\")\n",
        "\n",
        "#  JOIN com reparticionamento\n",
        "join_video_comments_repartition_OT = spark.sql(\"\"\"\n",
        "    SELECT v.*, c.*\n",
        "    FROM video_table_repartition v\n",
        "    JOIN comments_table_repartition c\n",
        "    ON v.video_id = c.video_id\n",
        "\"\"\")\n",
        "\n",
        "join_video_comments_repartition_OT.show()\n",
        "\n",
        "#  Aplicar coalesce\n",
        "df_video_coalesce_OT = df_video_cleaned_OT.coalesce(2)\n",
        "df_comments_coalesce_OT = df_comments_cleaned_OT.coalesce(2)\n",
        "\n",
        "#  Criar tabelas temporárias para os DataFrames coalesce\n",
        "df_video_coalesce_OT.createOrReplaceTempView(\"video_table_coalesce\")\n",
        "df_comments_coalesce_OT.createOrReplaceTempView(\"comments_table_coalesce\")\n",
        "\n",
        "#  Realizar o JOIN utilizando as tabelas temporárias coalesce\n",
        "join_video_comments_coalesce_OT = spark.sql(\"\"\"\n",
        "    SELECT v.*, c.*\n",
        "    FROM video_table_coalesce v\n",
        "    JOIN comments_table_coalesce c\n",
        "    ON v.video_id = c.video_id\n",
        "\"\"\")\n",
        "join_video_comments_coalesce_OT.show()\n",
        "\n",
        "#Utilize o explain para entender melhor as duas formas de realizar as etapas e refaça novamente as etapas anteriores, utilizando tudo que você já aprendeu para realizar o join e filter apenas com os dados necessários.\n",
        "\n",
        "# Visualizar as formas de realização:\n",
        "\n",
        "#Com repartition\n",
        "join_video_comments_repartition_OT.explain()\n",
        "\n",
        "#Com coalesce\n",
        "join_video_comments_coalesce_OT.explain()\n",
        "\n",
        "#Padronização dos dataframes\n",
        "\n",
        "df_video_cleaned_OT = df_video_OT.select(\n",
        "    *[col(c).alias(c.lower().replace(\" \", \"_\")) for c in df_video_OT.columns]\n",
        ")\n",
        "\n",
        "df_comments_cleaned_OT = df_comments_OT.select(\n",
        "    *[col(c).alias(c.lower().replace(\" \", \"_\")) for c in df_comments_OT.columns]\n",
        ")\n",
        "\n",
        "#  Selecionar apenas as colunas com dados necessários\n",
        "df_video_selected_OT = df_video_cleaned_OT.select(\"video_id\", \"title\", \"keyword\")\n",
        "df_comments_selected_OT = df_comments_cleaned_OT.select(\"video_id\", \"comment\", \"likes\")\n",
        "\n",
        "#  Filtrar apenas os dados não nulos\n",
        "df_video_filtered_OT = df_video_selected_OT.filter(\"video_id IS NOT NULL\")\n",
        "df_comments_filtered_OT = df_comments_selected_OT.filter(\"video_id IS NOT NULL\")\n",
        "\n",
        "#  Criar tabelas temporárias\n",
        "df_video_filtered_OT.createOrReplaceTempView(\"video_table\")\n",
        "df_comments_filtered_OT.createOrReplaceTempView(\"comments_table\")\n",
        "\n",
        "# Realização do JOIN\n",
        "join_video_comments_OT = spark.sql(\"\"\"\n",
        "    SELECT v.video_id, v.title, v.keyword, c.comment, c.likes\n",
        "    FROM video_table v\n",
        "    JOIN comments_table c\n",
        "    ON v.video_id = c.video_id\n",
        "\"\"\")\n",
        "#Mostra o resultado do JOIN\n",
        "join_video_comments_OT.show()\n",
        "\n",
        "# Salvando o DataFrame no formato Parquet\n",
        "join_video_comments_OT.write.option('header','true').mode(\"overwrite\").parquet(\"join-videos-comments-otimizado\")"
      ]
    }
  ]
}